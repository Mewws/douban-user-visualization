{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始爬取第0个用户！！！！\n",
      "['12555460', '莫吟思', 'https://img3.doubanio.com/icon/ul12555460-5.jpg', '2009-09-19', '北京', 38, 4318, 2, 6, 8, 0, 3, 33, '3', '62', 0]\n",
      "关注人数 38 被关注人数 4318\n",
      "关注人列表第4个用户: https://www.douban.com/people/58261194/\n",
      "开始爬取第1个用户！！！！\n",
      "['58261194', '书意', 'https://img3.doubanio.com/icon/ul58261194-4.jpg', '2012-02-03', '上海', 70, 1088, 0, 144, 95, 0, 1, 25, '4', '82', 0]\n",
      "关注人数 70 被关注人数 1088\n",
      "关注人列表第37个用户: https://www.douban.com/people/3790983/\n",
      "开始爬取第2个用户！！！！\n",
      "['3790983', '齐物秋水', 'https://img3.doubanio.com/icon/ul3790983-100.jpg', '2009-03-11', '北京', 312, 14504, 0, 161, 1045, 0, 10, 506, '304', '23', 0]\n",
      "关注人数 312 被关注人数 14504\n",
      "关注人列表第272个用户: https://www.douban.com/people/3319131/\n",
      "开始爬取第3个用户！！！！\n",
      "['3319131', '邝海炎', 'https://img3.doubanio.com/icon/ul3319131-2.jpg', '2008-12-09', '广东广州', 582, 1513, 242, 1992, 1227, 0, 4, 84, '46', '90', 0]\n",
      "关注人数 582 被关注人数 1513\n",
      "被关注人列表第31个用户: https://www.douban.com/people/179383126/\n",
      "开始爬取第4个用户！！！！\n",
      "['179383126', '颜轻', 'https://img3.doubanio.com/icon/ul179383126-1.jpg', '2018-06-01', '广东深圳', 2, 2, 0, 0, 0, 0, 1, 0, 0, '66', 0]\n",
      "关注人数 2 被关注人数 2\n",
      "关注人列表第1个用户: https://www.douban.com/people/175930019/\n",
      "开始爬取第5个用户！！！！\n",
      "['175930019', '蛮馒', 'https://img3.doubanio.com/icon/ul175930019-3.jpg', '2018-03-26', '广东深圳', 6, 6, 0, 0, 0, 0, 1, 0, 0, '52', 0]\n",
      "关注人数 6 被关注人数 6\n",
      "被关注人列表第4个用户: https://www.douban.com/people/174867730/\n",
      "开始爬取第6个用户！！！！\n",
      "['174867730', '六乐苍穹', 'https://img1.doubanio.com/icon/user_normal.jpg', '2018-03-03', '', 1, 1, 0, 0, 0, 0, 0, 0, 0, '9', 0]\n",
      "关注人数 1 被关注人数 1\n",
      "被关注人列表第0个用户: https://www.douban.com/people/179001097/\n",
      "开始爬取第7个用户！！！！\n",
      "['179001097', '李先生', 'https://img1.doubanio.com/icon/user_normal.jpg', '2018-05-23', '广东深圳', 15, 3, 0, 0, 0, 0, 4, 0, 0, '32', 0]\n",
      "关注人数 15 被关注人数 3\n",
      "被关注人列表第0个用户: https://www.douban.com/people/183631998/\n",
      "开始爬取第8个用户！！！！\n",
      "['183631998', 'L森', 'https://img3.doubanio.com/icon/ul183631998-1.jpg', '2018-08-26', '', 5, 0, 0, 0, 0, 0, 0, 6, 0, '9', 0]\n",
      "关注人数 5 被关注人数 0\n",
      "关注人列表第2个用户: https://www.douban.com/people/180441686/\n",
      "开始爬取第9个用户！！！！\n",
      "['180441686', '新片情报局', 'https://img3.doubanio.com/icon/ul180441686-1.jpg', '2018-06-28', '北京', 11, 1000000, 0, 1, 3, 0, 0, 11, 0, 0, 1]\n",
      "关注人数 11 被关注人数 1000000\n",
      "关注人列表第3个用户: https://www.douban.com/people/180457410/\n"
     ]
    }
   ],
   "source": [
    "#1.寻找目标网站并分析结构\n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "header=['userid','username','userimg','create_date','residence','contact_num','rev_contact_num','book_do_num','book_wish_num','book_collect_num','movie_do_num','movie_wish_num','movie_collect_num','review_num','group_num','official_account']\n",
    "with open('./crawler/doban.csv','w',encoding='utf-8') as f:\n",
    "    f_csv=csv.writer(f)\n",
    "    f_csv.writerow(header)\n",
    "    \n",
    "#定义爬虫的User Agent\n",
    "user_agent=user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "cookie='viewed=\"30294837\"; bid=akwf19Sj9vg; gr_user_id=e3f576be-96b7-4d23-8f81-5524f9ea9900; _vwo_uuid_v2=D9E83E42E6E6F8789BF8EDD449B9A896A|ae27072bc60e0a6c4907422d17bc74df; ll=\"118173\"; __utmc=30149280; __utmz=30149280.1544341287.3.3.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; ap_v=0,6.0; __utma=30149280.1597611217.1543150549.1544346180.1544355300.5; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1544357837%2C%22https%3A%2F%2Fmovie.douban.com%2Fsubject%2F24773958%2F%22%5D; _pk_ses.100001.8cb4=*; as=\"https://www.douban.com/people/lingrui1995/\"; ps=y; dbcl2=\"174075919:oOlXd2Fnio4\"; ck=ztF5; push_noty_num=0; push_doumail_num=0; __utmv=30149280.17407; douban-profile-remind=1; __yadk_uid=BLaoiyly2p2NOMHSBlhlWukjs9OBFF1x; douban-fav-remind=1; ct=y; __utmt=1; _pk_id.100001.8cb4=966b8455d2cbac96.1543646893.3.1544360056.1544341668.; __utmb=30149280.65.10.1544355300'\n",
    "headers={'User-Agent':user_agent,'Cookie':cookie}\n",
    "\n",
    "MAX=10\n",
    "init_url='https://www.douban.com/people/12555460/'\n",
    "\n",
    "def getUserInfo(user_url,count,last_user_url):\n",
    "    print('开始爬取第{}个用户！！！！'.format(count))\n",
    "    #使用requests发送请求\n",
    "    user=requests.get(user_url,headers=headers)\n",
    "    user.encoding='utf-8'\n",
    "    soup=BeautifulSoup(user.text,'lxml')\n",
    "    \n",
    "    # 判断用户是否有效\n",
    "    infobox=soup.find('div',class_='mn')\n",
    "    if infobox:\n",
    "        getUserInfo(last_user_url,count,last_user_url)\n",
    "        return 0\n",
    "        #isvalid=re.search(r'注销帐号',infobox.get_text()) #用户是否注销账\n",
    "        #if isvalid:\n",
    "        #    getUserInfo(last_user_url,count,last_user_url)\n",
    "        #    return 0\n",
    "    #获取用户基本信息\n",
    "    all_info=[]\n",
    "    #id\n",
    "    userid=re.split(r'/',user_url)[-2]\n",
    "    #用户名\n",
    "    img=soup.find('div',class_='pic').find('img')\n",
    "    if img:\n",
    "        username=img.get('alt') \n",
    "    else:\n",
    "        username=soup.find('div',class_='pic').find('a').get('title')\n",
    "    #头像\n",
    "    userimg=soup.find('div',class_='basic-info').find('img').get('src')\n",
    "    \n",
    "    #加入时间\n",
    "    userinfo=soup.find('div',class_='user-info').find('div',class_='pl').get_text()\n",
    "    create_date=re.search(r'(\\d{4}-\\d{2}-\\d{2})',userinfo).group()\n",
    "    #常居地\n",
    "    residence1=soup.find('div',class_='user-info').find('a')\n",
    "    residence=residence1.get_text() if residence1 else ''\n",
    "    #是否是豆瓣官方运营帐号\n",
    "    is_reason=soup.find('span',class_='reason')\n",
    "    official_account=1 if is_reason else 0\n",
    "    #书的数量\n",
    "    book_do_num=0\n",
    "    book_wish_num=0\n",
    "    book_collect_num=0\n",
    "    book=soup.find('div',id='book').find('span',class_='pl')\n",
    "    if book:\n",
    "        books=book.find_all('a')\n",
    "        for i in range(len(books)):\n",
    "            has_do=re.search('/do',books[i].get('href'))\n",
    "            if has_do:  \n",
    "                book_do_num=int(re.match(r'\\d+',books[i].get_text()).group())\n",
    "                continue\n",
    "            has_wish=re.search('/wish',books[i].get('href'))\n",
    "            if has_wish:\n",
    "                book_wish_num=int(re.match(r'\\d+',books[i].get_text()).group())\n",
    "                continue   \n",
    "            has_collect=re.search('/collect',books[i].get('href'))\n",
    "            if has_collect:\n",
    "                book_collect_num=int(re.match(r'\\d+',books[i].get_text()).group())\n",
    "                continue\n",
    "    #电影数量\n",
    "    movie_do_num=0\n",
    "    movie_wish_num=0\n",
    "    movie_collect_num=0\n",
    "    movie=soup.find('div',id='movie').find('span',class_='pl')\n",
    "    if movie:\n",
    "        movies=movie.find_all('a')\n",
    "        for i in range(len(movies)):\n",
    "            has_do=re.search('/do',movies[i].get('href'))\n",
    "            if has_do:  \n",
    "                movie_do_num=int(re.match(r'\\d+',movies[i].get_text()).group())\n",
    "                continue\n",
    "            has_wish=re.search('/wish',movies[i].get('href'))\n",
    "            if has_wish:\n",
    "                movie_wish_num=int(re.match(r'\\d+',movies[i].get_text()).group())\n",
    "                continue   \n",
    "            has_collect=re.search('/collect',movies[i].get('href'))\n",
    "            if has_collect:\n",
    "                movie_collect_num=int(re.match(r'\\d+',movies[i].get_text()).group())\n",
    "                continue\n",
    "    #评论数量\n",
    "    review=soup.find('div',id='review').find('a')\n",
    "    review_num=re.search(r'\\d+',review.get_text()).group() if review else 0\n",
    "    #常去小组数量\n",
    "    group=soup.find('div',id='group').find('h2')\n",
    "    group_num=re.findall('\\((\\d+)\\)',group.get_text())[0] if group else 0\n",
    "    \n",
    "    #关注人数\n",
    "    contact=soup.find('div',id='friend').find('a').get_text()\n",
    "    contact_num=int(re.findall(r'成员(\\d+)',contact)[0]) #关注人数\n",
    "    \n",
    "    #被关注人数\n",
    "    rev_contact=soup.find('p',class_='rev-link').find('a').get_text()\n",
    "    rev_contact_num=int(re.findall(r'被(\\d+)',rev_contact)[0]) #被关注人数\n",
    "    \n",
    "    all_info.extend([userid,username,userimg,create_date,residence,contact_num,rev_contact_num,book_do_num,book_wish_num,book_collect_num,movie_do_num,movie_wish_num,movie_collect_num,review_num,group_num,official_account])\n",
    "    print(all_info)\n",
    "    if (count==0) | (user_url!=last_user_url):\n",
    "        with open('./crawler/doban.csv','a',encoding='utf-8') as f:\n",
    "            f_csv=csv.writer(f)\n",
    "            f_csv.writerow(all_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 开始爬取下一个用户\n",
    "    if contact_num & rev_contact_num:\n",
    "        contact_or_rev=random.sample(range(0,2),1)[0]\n",
    "    else:\n",
    "        contact_or_rev = 0 if contact_num > 0 else 1\n",
    "        \n",
    "    print(\"关注人数\",contact_num,\"被关注人数\",rev_contact_num)\n",
    "        \n",
    "    #进入关注人列表\n",
    "    if contact_or_rev == 0:\n",
    "        n=random.randint(0,contact_num-1) #随机获取列表中第n个用户 \n",
    "        contact_text=requests.get(user_url+'/contacts',headers=headers).text\n",
    "        \n",
    "        contact_soup=BeautifulSoup(contact_text,'lxml')\n",
    "        try:\n",
    "            obu=contact_soup.find_all('dl',class_='obu')[n]\n",
    "            obu_url=obu.find('a').get('href')\n",
    "        except:\n",
    "            print(n)\n",
    "            print(contact_soup.find_all('dl',class_='obu'))\n",
    "            \n",
    "        print('关注人列表第{}个用户:'.format(n),obu_url)\n",
    "    #进入被关注人列表\n",
    "    else:\n",
    "        n=random.randint(0,rev_contact_num-1) #随机获取列表中第n个用户 \n",
    "        rev_contact_text=requests.get(user_url+'/rev_contacts?start='+str(n//70*70),headers=headers).text\n",
    "        rev_contact_soup=BeautifulSoup(rev_contact_text,'lxml')\n",
    "        try:\n",
    "            obu=rev_contact_soup.find_all('dl',class_='obu')[n%70-1]\n",
    "            obu_url=obu.find('a').get('href')\n",
    "        except:\n",
    "            print(n)\n",
    "            print(rev_contact_soup.find_all('dl',class_='obu'))\n",
    "        print('被关注人列表第{}个用户:'.format(n),obu_url)\n",
    "    \n",
    "    # 递归调用\n",
    "    count+=1\n",
    "    if count < MAX:\n",
    "        getUserInfo(obu_url,count,user_url)\n",
    "        \n",
    "getUserInfo(init_url,0,init_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "被关注人列表第7个用户: https://www.douban.com/people/46741762/\n"
     ]
    }
   ],
   "source": [
    "rev_contact_text=requests.get('https://www.douban.com/people/70032148/',headers=headers).text\n",
    "n=random.randint(0,-1) #随机获取列表中第n个用户 \n",
    "rev_contact_soup=BeautifulSoup(rev_contact_text,'lxml')\n",
    "obu=rev_contact_soup.find_all('dl',class_='obu')[n]\n",
    "obu_url=obu.find('a').get('href')\n",
    "print('被关注人列表第{}个用户:'.format(n),obu_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=random.sample(range(0,3),1)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=random.randint(0,3)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_text=requests.get('https://www.douban.com/people/85359370','/contacts',headers=headers).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "user=requests.get('https://www.douban.com/people/180441686/',headers=headers)\n",
    "user.encoding='utf-8'\n",
    "soup=BeautifulSoup(user.text,'lxml')\n",
    "#用户基本信息\n",
    "#头像\n",
    "userimg=soup.find('div',class_='basic-info').find('img').get('src')\n",
    "#用户名\n",
    "#username=soup.find('div',class_='pic').find('img').get('alt')\n",
    "username=soup.find('div',class_='pic').find('a').get('title')\n",
    "print(username)\n",
    "#id\n",
    "userinfo=soup.find('div',class_='user-info').find('div',class_='pl').get_text()\n",
    "userid=re.match(r'\\d+',userinfo).group()\n",
    "#加入时间\n",
    "create_date=re.search(r'(\\d{4}-\\d{2}-\\d{2})',userinfo).group()\n",
    "#常居地\n",
    "residence1=soup.find('div',class_='user-info').find('a')\n",
    "residence=residence1.get_text() if residence1 else ''\n",
    "#是否是豆瓣官方运营帐号\n",
    "is_reason=soup.find('span',class_='reason')\n",
    "#reason=re.match(r'豆瓣官方运营帐号',soup.find('span',class_='reason').string)\n",
    "official_account=1 if is_reason else 0\n",
    "print(official_account)\n",
    "\n",
    "\n",
    "#书的数量\n",
    "book_do_num=0\n",
    "book_wish_num=0\n",
    "book_collect_num=0\n",
    "\n",
    "book=soup.find('div',id='book').find('span',class_='pl').find_all('a')\n",
    "for i in range(len(book)):\n",
    "    has_do=re.search('/do',book[i].get('href'))\n",
    "    if has_do:  \n",
    "        book_do_num=re.match(r'\\d+',book[i].get_text()).group()\n",
    "        continue\n",
    "    has_wish=re.search('/wish',book[i].get('href'))\n",
    "    if has_wish:\n",
    "        book_wish_num=re.match(r'\\d+',book[i].get_text()).group()\n",
    "        continue   \n",
    "    has_collect=re.search('/collect',book[i].get('href'))\n",
    "    if has_collect:\n",
    "        book_collect_num=re.match(r'\\d+',book[i].get_text()).group()\n",
    "        continue\n",
    "\n",
    "\n",
    "#电影数量\n",
    "movie_do_num=0\n",
    "movie_wish_num=0\n",
    "movie_collect_num=0\n",
    "\n",
    "movie=soup.find('div',id='movie').find('span',class_='pl').find_all('a')\n",
    "for i in range(len(movie)):\n",
    "    has_do=re.search('/do',movie[i].get('href'))\n",
    "    if has_do:  \n",
    "        movie_do_num=re.match(r'\\d+',movie[i].get_text()).group()\n",
    "        continue\n",
    "    has_wish=re.search('/wish',movie[i].get('href'))\n",
    "    if has_wish:\n",
    "        movie_wish_num=re.match(r'\\d+',movie[i].get_text()).group()\n",
    "        continue   \n",
    "    has_collect=re.search('/collect',movie[i].get('href'))\n",
    "    if has_collect:\n",
    "        movie_collect_num=re.match(r'\\d+',movie[i].get_text()).group()\n",
    "        continue\n",
    "\n",
    "#评论数量\n",
    "#review=soup.find('div',id='review').find('a').get_text()\n",
    "#review_num=re.search(r'\\d+',review).group() if review else 0\n",
    "#常去小组数量\n",
    "\n",
    "#group=soup.find('div',id='group').find('h2').get_text()\n",
    "group=None\n",
    "group_num=re.findall('\\((\\d+)\\)',group)[0] if group else 0\n",
    "#print(group_num)\n",
    "all_info=[]\n",
    "all_info.extend([userid,username,userimg,create_date,residence,book_do_num,book_wish_num,book_collect_num,movie_do_num,movie_wish_num,movie_collect_num,review_num,group_num])\n",
    "#print(all_info)\n",
    "\n",
    "\n",
    "\n",
    "#print(movie_do_num,movie_wish_num,movie_collect_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=1\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,0]\n",
    "for i in range(3):\n",
    "    if a[i]==1:\n",
    "        print('a=1')\n",
    "        continue\n",
    "    if a[i]==2:\n",
    "        a[i]=22\n",
    "        print(a[i])\n",
    "        continue\n",
    "    if a[i]==3:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "x=re.findall('\\((\\d+)\\)','莫122吟思常去的小组(62)')\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "https://book.douban.com/people/12555460/wish\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "has_do=None\n",
    "print(has_do)\n",
    "has_do=re.search('/do',book[1].get('href'))\n",
    "print(book[1].get('href'))\n",
    "print(has_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 9, 0, 0)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date='2018-12-09'\n",
    "create_date=datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "create_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purpledreamer\n"
     ]
    }
   ],
   "source": [
    "userid=re.split(r'/','https://www.douban.com/people/purpledreamer/')[-2]\n",
    "print(userid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "for i in range(3):\n",
    "    row=[1+i,2+i,3+i]\n",
    "    with open('./crawler/test.csv','a',encoding='utf-8') as f:\n",
    "        f_csv=csv.writer(f)\n",
    "        f_csv.writerow(row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "header=['发文部门','标题','链接']\n",
    "with open('./crawler/test.csv','w',encoding='utf-8') as f:\n",
    "    f_csv=csv.writer(f)\n",
    "    f_csv.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
